{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# These are taken from https://github.com/mila-udem/blocks\n",
    "# \n",
    "\n",
    "class Constant():\n",
    "    \"\"\"Initialize parameters to a constant.\n",
    "    The constant may be a scalar or a :class:`~numpy.ndarray` of any shape\n",
    "    that is broadcastable with the requested parameter arrays.\n",
    "    Parameters\n",
    "    ----------\n",
    "    constant : :class:`~numpy.ndarray`\n",
    "        The initialization value to use. Must be a scalar or an ndarray (or\n",
    "        compatible object, such as a nested list) that has a shape that is\n",
    "        broadcastable with any shape requested by `initialize`.\n",
    "    \"\"\"\n",
    "    def __init__(self, constant):\n",
    "        self._constant = numpy.asarray(constant)\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        dest = numpy.empty(shape, dtype=np.float32)\n",
    "        dest[...] = self._constant\n",
    "        return dest\n",
    "\n",
    "\n",
    "class IsotropicGaussian():\n",
    "    \"\"\"Initialize parameters from an isotropic Gaussian distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    std : float, optional\n",
    "        The standard deviation of the Gaussian distribution. Defaults to 1.\n",
    "    mean : float, optional\n",
    "        The mean of the Gaussian distribution. Defaults to 0\n",
    "    Notes\n",
    "    -----\n",
    "    Be careful: the standard deviation goes first and the mean goes\n",
    "    second!\n",
    "    \"\"\"\n",
    "    def __init__(self, std=1, mean=0):\n",
    "        self._mean = mean\n",
    "        self._std = std\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        m = rng.normal(self._mean, self._std, size=shape)\n",
    "        #print(np.sum(m > 0.))\n",
    "        #print(np.sum(m == 0.))\n",
    "        #print(np.sum(m < 0.))\n",
    "        return m.astype(np.float32)\n",
    "\n",
    "\n",
    "class Uniform():\n",
    "    \"\"\"Initialize parameters from a uniform distribution.\n",
    "    Parameters\n",
    "    ----------\n",
    "    mean : float, optional\n",
    "        The mean of the uniform distribution (i.e. the center of mass for\n",
    "        the density function); Defaults to 0.\n",
    "    width : float, optional\n",
    "        One way of specifying the range of the uniform distribution. The\n",
    "        support will be [mean - width/2, mean + width/2]. **Exactly one**\n",
    "        of `width` or `std` must be specified.\n",
    "    std : float, optional\n",
    "        An alternative method of specifying the range of the uniform\n",
    "        distribution. Chooses the width of the uniform such that random\n",
    "        variates will have a desired standard deviation. **Exactly one** of\n",
    "        `width` or `std` must be specified.\n",
    "    \"\"\"\n",
    "    def __init__(self, mean=0., width=None, std=None):\n",
    "        if (width is not None) == (std is not None):\n",
    "            raise ValueError(\"must specify width or std, \"\n",
    "                             \"but not both\")\n",
    "        if std is not None:\n",
    "            # Variance of a uniform is 1/12 * width^2\n",
    "            self._width = numpy.sqrt(12) * std\n",
    "        else:\n",
    "            self._width = width\n",
    "        self._mean = mean\n",
    "\n",
    "    def generate(self, rng, shape):\n",
    "        w = self._width / 2\n",
    "        m = rng.uniform(self._mean - w, self._mean + w, size=shape)\n",
    "        return m.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_im2col_indices(x_shape, field_height, field_width, padding=1, stride=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    N, C, H, W = x_shape\n",
    "    assert (H + 2 * padding - field_height) % stride == 0\n",
    "    assert (W + 2 * padding - field_height) % stride == 0\n",
    "    out_height = int((H + 2 * padding - field_height) / stride + 1)\n",
    "    out_width = int((W + 2 * padding - field_width) / stride + 1)\n",
    "\n",
    "    i0 = np.repeat(np.arange(field_height), field_width)\n",
    "    i0 = np.tile(i0, C)\n",
    "    i1 = stride * np.repeat(np.arange(out_height), out_width)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    j1 = stride * np.tile(np.arange(out_width), out_height)\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C), field_height * field_width).reshape(-1, 1)\n",
    "\n",
    "    return (k.astype(int), i.astype(int), j.astype(int))\n",
    "\n",
    "\n",
    "def im2col_indices(x, field_height, field_width, padding=1, stride=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    x_padded = np.pad(x, ((0, 0), (0, 0), (p, p), (p, p)), mode='constant')\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride)\n",
    "\n",
    "    cols = x_padded[:, k, i, j]\n",
    "    C = x.shape[1]\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    return cols\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1,\n",
    "                   stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, C, H, W = x_shape\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding, stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n",
    "\n",
    "class Layer(object):\n",
    "    def __init__(self, rng=None):\n",
    "        if rng is None:\n",
    "            rng = numpy.random\n",
    "        self.rng = rng\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return []\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return []\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        return [] \n",
    "\n",
    "class AffineLayer(Layer):\n",
    "    def __init__(self, num_in, num_out, weight_init=None, bias_init=None, **kwargs):\n",
    "        super(AffineLayer, self).__init__(**kwargs)\n",
    "        if weight_init is None:\n",
    "            weight_init = IsotropicGaussian(std=0.2, mean=0.0)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "\n",
    "        self.W = weight_init.generate(self.rng, (num_out, num_in))\n",
    "        self.b = bias_init.generate(self.rng, (num_out, 1))\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        fprop_context = dict(X=X)\n",
    "        Y = np.dot(self.W, X) + self.b\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        return self.W.T.dot(dLdY)\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        dLdW = np.dot(dLdY, X.T)\n",
    "        dLdb = dLdY.sum(1, keepdims=True)\n",
    "        return [dLdW, dLdb]\n",
    "\n",
    "\n",
    "class TanhLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(TanhLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.tanh(X)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        return dLdY * (1.0 - Y**2)\n",
    "\n",
    "    \n",
    "class ReLULayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(ReLULayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        Y = np.maximum(X, 0.0)\n",
    "        fprop_context = dict(Y=Y)\n",
    "        return Y, fprop_context\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        Y = fprop_context['Y']\n",
    "        dLdX = dLdY.reshape(Y.shape) * (Y > 0)\n",
    "        return dLdX\n",
    "\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def compute_probabilities(self, X):\n",
    "        O = X - X.max(axis=0, keepdims=True)\n",
    "        O = np.exp(O)\n",
    "        O /= O.sum(axis=0, keepdims=True)\n",
    "        return O\n",
    "    \n",
    "    def fprop_cost(self, X, Y):\n",
    "        NS = X.shape[1]\n",
    "        O = self.compute_probabilities(X)\n",
    "        Cost = -1.0/NS * np.log(O[Y.ravel(), range(NS)]).sum()\n",
    "        return Cost, O, dict(O=O, X=X, Y=Y)\n",
    "    \n",
    "    def bprop_cost(self, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        Y = fprop_context['Y']\n",
    "        O = fprop_context['O']\n",
    "        NS = X.shape[1]\n",
    "        dLdX = O.copy()\n",
    "        dLdX[Y, range(NS)] -= 1.0\n",
    "        dLdX /= NS\n",
    "        return dLdX\n",
    "    \n",
    "class FeedForwardNet(object):\n",
    "    def __init__(self, layers=None):\n",
    "        if layers is None:\n",
    "            layers = []\n",
    "        self.layers = layers\n",
    "    \n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "    \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        params = []\n",
    "        for layer in self.layers:\n",
    "            params += layer.parameters\n",
    "        return params\n",
    "    \n",
    "    @parameters.setter\n",
    "    def parameters(self, values):\n",
    "        for ownP, newP in zip(self.parameters, values):\n",
    "            ownP[...] = newP\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        param_names = []\n",
    "        for layer in self.layers:\n",
    "            param_names += layer.parameter_names\n",
    "        return param_names\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "        return self.layers[-1].compute_probabilities(X)\n",
    "    \n",
    "    def get_cost_and_gradient(self, X, Y):\n",
    "        fp_contexts = []\n",
    "        for layer in self.layers[:-1]:\n",
    "            X, fp_context = layer.fprop(X)\n",
    "            fp_contexts.append(fp_context)\n",
    "        \n",
    "        L, O, fp_context = self.layers[-1].fprop_cost(X, Y)\n",
    "        dLdX = self.layers[-1].bprop_cost(fp_context)\n",
    "        \n",
    "        dLdP = [] #gradient with respect to parameters\n",
    "        for i in xrange(len(self.layers)-1):\n",
    "            layer = self.layers[len(self.layers)-2-i]\n",
    "            fp_context = fp_contexts[len(self.layers)-2-i]\n",
    "            dLdP = layer.get_gradients(dLdX, fp_context) + dLdP\n",
    "            dLdX = layer.bprop(dLdX, fp_context)\n",
    "            \n",
    "        return L, O, dLdP\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "   \n",
    "class PoolLayer(Layer):\n",
    "    def __init__(self, image_shape, **kwargs):\n",
    "        super(PoolLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        self.poolsize = 2\n",
    "        self.stride = 2\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        num_images, num_channels, img_w, img_h = self.image_shape\n",
    "\n",
    "        Xr = X.reshape(num_images * num_filters, 1, img_h, img_w)\n",
    "        X_pool = im2col_indices(Yr, self.poolsize, self.poolsize, stride = self.stride, padding=0)\n",
    "        #print X_pool.shape\n",
    "\n",
    "        max_idx = np.argmax(X_pool, axis=0)\n",
    "        #print max_idx.shape\n",
    "        Y_pool = X_pool[max_idx, range(max_idx.size)]\n",
    "        #print Y_pool.shape\n",
    "\n",
    "        Y_pool = Y_pool.reshape(img_h / self.stride, img_w / self.stride, n, d)\n",
    "        #print Y_pool.shape\n",
    "\n",
    "        Y_pool = Y_pool.transpose(2, 3, 0, 1)\n",
    "        \n",
    "        Y_pool = Y_pool.reshape(num_images, -1).T\n",
    "        \n",
    "        return Y_pool, dict(X=Xr, X_pool=X_pool, max_idx = max_idx)\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        max_idx = fprop_context['max_idx']\n",
    "        X_col = fprop_context['X_pool']\n",
    "\n",
    "        n, d, w, h = X.shape\n",
    "\n",
    "        dX_col = np.zeros_like(X_col)\n",
    "        \n",
    "        dout_col = dLdY.ravel()\n",
    "\n",
    "        dX_col[max_idx, range(dout_col.size)] = dout_col\n",
    "        dX = dX_col\n",
    "\n",
    "        dX = col2im_indices(dX_col, (n * d, 1, h, w), self.poolsize, self.poolsize, padding=0, stride=self.stride)\n",
    "        dX = dX.reshape(X.shape)\n",
    "\n",
    "        return dX\n",
    "    \n",
    "class ConvPoolLayer(Layer):\n",
    "    def __init__(self, image_shape, filter_shape, weight_init = None, bias_init = None, **kwargs):\n",
    "        super(ConvPoolLayer, self).__init__(**kwargs)\n",
    "        \n",
    "        if weight_init is None:\n",
    "            weight_init = IsotropicGaussian(std=0.2, mean=0.0)\n",
    "        if bias_init is None:\n",
    "            bias_init = Constant(0.0)\n",
    "        \n",
    "        self.image_shape = image_shape\n",
    "        self.filter_shape = filter_shape\n",
    "        \n",
    "        self.W = weight_init.generate(self.rng, filter_shape)\n",
    "        self.b = weight_init.generate(self.rng, (filter_shape[0],))\n",
    "        \n",
    "    @property\n",
    "    def parameters(self):\n",
    "        return [self.W, self.b]\n",
    "    \n",
    "    @property\n",
    "    def parameter_names(self):\n",
    "        return ['W','b']\n",
    "    \n",
    "    def fprop(self, X):\n",
    "        num_images, num_channels, img_w, img_h = self.image_shape\n",
    "        num_filters, num_channels2, filter_w, filter_h = self.filter_shape\n",
    "\n",
    "        Xr = X.reshape(self.image_shape)\n",
    "        X_col = im2col_indices(Xr, filter_w, filter_h, stride = 1, padding=0)\n",
    "\n",
    "        #print X_col.shape\n",
    "\n",
    "        W_col = W.reshape(num_filters, -1)\n",
    "        #print W_col.shape\n",
    "\n",
    "        Y = W_col.dot(X_col) + b\n",
    "        #Y = np.maximum(Y, 0.0) # ReLU\n",
    "        #print Y.shape\n",
    "\n",
    "        w_out = img_w - filter_w + 1\n",
    "        h_out = img_h - filter_h + 1\n",
    "\n",
    "        Y = Y.reshape(num_filters, w_out, h_out, num_images)\n",
    "        #print Y.shape\n",
    "\n",
    "        Y = Y.transpose(3, 0, 1, 2)\n",
    "        #print Y.shape\n",
    "        \n",
    "        return Y, dict(X=Xr, X_col=X_col)\n",
    "    \n",
    "    def bprop(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        #Xr = fprop_context['Xr']\n",
    "        X_col = fprop_context['X_col']\n",
    "        #X, W, b, stride, padding, X_col = cache\n",
    "        n_filter, d_filter, h_filter, w_filter = self.W.shape\n",
    "\n",
    "        dout_reshaped = dLdY.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "\n",
    "        W_reshape = self.W.reshape(n_filter, -1)\n",
    "        dX_col = W_reshape.T.dot(dout_reshaped)\n",
    "        dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=0, stride=1)\n",
    "\n",
    "        return dX #, dW, db\n",
    "    \n",
    "    def get_gradients(self, dLdY, fprop_context):\n",
    "        X = fprop_context['X']\n",
    "        X_col = fprop_context['X_col']\n",
    "        #X, W, b, stride, padding, X_col = cache\n",
    "        n_filter, d_filter, h_filter, w_filter = self.W.shape\n",
    "        \n",
    "        db = np.sum(dLdY, axis=(0, 2, 3))\n",
    "        \n",
    "        db = db.reshape(n_filter, -1).ravel()\n",
    "\n",
    "        dout_reshaped = dLdY.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "        dW = dout_reshaped.dot(X_col.T)\n",
    "        dW = dW.reshape(W.shape)\n",
    "\n",
    "        return [dW, db]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network configuration: \n",
      "W(20, 1, 5, 5), b(20,), W(10, 2880), b(10, 1)\n",
      "At minibatch 100, batch loss 2.381078, batch error rate 90.000000%\n",
      "At minibatch 200, batch loss 2.413829, batch error rate 93.000000%\n",
      "At minibatch 300, batch loss 2.340055, batch error rate 91.000000%\n",
      "At minibatch 400, batch loss 2.360413, batch error rate 91.000000%\n",
      "At minibatch 500, batch loss 2.304931, batch error rate 86.000000%\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "total size of new array must be unchanged",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-b1da87a046f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mregularization_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0malpha_alg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlphaAlgExp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.992\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     momentum_alg = MomentumAlg3())\n\u001b[0m",
      "\u001b[0;32m<ipython-input-144-19edf286a134>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(net, train_stream, validation_stream, test_stream, print_debug, epochs, patience, alpha_alg, momentum_alg, regularization_rate, dropout)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;31m# After an epoch compute validation error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             \u001b[0mval_error_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_error_rate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mval_error_rate\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mbest_valid_error_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-144-19edf286a134>\u001b[0m in \u001b[0;36mcompute_error_rate\u001b[0;34m(net, stream)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_epoch_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mnum_errs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mnum_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-129-380bf1371202>\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_probabilities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-147-a224501a0c78>\u001b[0m in \u001b[0;36mfprop\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     75\u001b[0m      \u001b[0mnum_filters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m      \u001b[0mXr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m      \u001b[0mX_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim2col_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: total size of new array must be unchanged"
     ]
    }
   ],
   "source": [
    "weight_init = IsotropicGaussian(std=0.05, mean=0.0)\n",
    "net = FeedForwardNet([\n",
    "        ConvPoolLayer(image_shape=(100, 1, 28, 28), filter_shape=(20, 1, 5, 5)),\n",
    "        ReLULayer(),\n",
    "        PoolLayer(image_shape=(100, 20, 24, 24)),\n",
    "        AffineLayer(20 * 12 * 12, 10, weight_init = weight_init),\n",
    "        SoftMaxLayer()\n",
    "    ])\n",
    "\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream,\n",
    "    print_debug = True,\n",
    "    epochs = 2000,\n",
    "    regularization_rate = 0.0,\n",
    "    alpha_alg = AlphaAlgExp(initial = 5e-2, rate = 0.992),\n",
    "    momentum_alg = MomentumAlg3())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from fuel.datasets.mnist import MNIST\n",
    "from fuel.transformers import ScaleAndShift, Cast, Flatten, Mapping\n",
    "from fuel.streams import DataStream\n",
    "from fuel.schemes import SequentialScheme, ShuffledScheme\n",
    "\n",
    "MNIST.default_transformers = (\n",
    "    (ScaleAndShift, [2.0 / 255.0, -1], {'which_sources': 'features'}),\n",
    "    (Cast, [np.float32], {'which_sources': 'features'}), \n",
    "    (Flatten, [], {'which_sources': 'features'}),\n",
    "    (Mapping, [lambda batch: (b.T for b in batch)], {}) )\n",
    "\n",
    "mnist_train = MNIST((\"train\",), subset=slice(None,50000))\n",
    "#this stream will shuffle the MNIST set and return us batches of 100 examples\n",
    "mnist_train_stream = DataStream.default_stream(\n",
    "    mnist_train,\n",
    "    iteration_scheme=ShuffledScheme(mnist_train.num_examples, 100))\n",
    "                                               \n",
    "mnist_validation = MNIST((\"train\",), subset=slice(50000, None))\n",
    "\n",
    "# We will use larger portions for testing and validation\n",
    "# as these dont do a backward pass and reauire less RAM.\n",
    "mnist_validation_stream = DataStream.default_stream(\n",
    "    mnist_validation, iteration_scheme=SequentialScheme(mnist_validation.num_examples, 250))\n",
    "mnist_test = MNIST((\"test\",))\n",
    "mnist_test_stream = DataStream.default_stream(\n",
    "    mnist_test, iteration_scheme=SequentialScheme(mnist_test.num_examples, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The streams return batches containing (u'features', u'targets')\n",
      "Each trainin batch consits of a tuple containing:\n",
      " - an array of size (784, 100) containing float32\n",
      " - an array of size (1, 100) containing uint8\n",
      "Validation/test batches consits of tuples containing:\n",
      " - an array of size (784, 250) containing float32\n",
      " - an array of size (1, 250) containing uint8\n"
     ]
    }
   ],
   "source": [
    "print \"The streams return batches containing %s\" % (mnist_train_stream.sources,)\n",
    "\n",
    "print \"Each trainin batch consits of a tuple containing:\"\n",
    "for element in next(mnist_train_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)\n",
    "    \n",
    "print \"Validation/test batches consits of tuples containing:\"\n",
    "for element in next(mnist_test_stream.get_epoch_iterator()):\n",
    "    print \" - an array of size %s containing %s\" % (element.shape, element.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def compute_error_rate(net, stream):\n",
    "    num_errs = 0.0\n",
    "    num_examples = 0\n",
    "            \n",
    "    for X, Y in stream.get_epoch_iterator():\n",
    "        O = net.fprop(X)\n",
    "        num_errs += (O.argmax(0) != Y).sum()\n",
    "        num_examples += X.shape[1]\n",
    "            \n",
    "    return num_errs/num_examples\n",
    "\n",
    "def print_stats(train_loss, train_errors, validation_errors):\n",
    "    subplot(2,1,1)\n",
    "    train_loss = np.array(train_loss)\n",
    "    semilogy(train_loss[:,0], train_loss[:,1], label='batch train loss')\n",
    "    legend()\n",
    "\n",
    "    subplot(2,1,2)\n",
    "    train_errors = np.array(train_errors)\n",
    "    plot(train_errors[:,0], train_errors[:,1], label='batch train error rate')\n",
    "    validation_errors = np.array(validation_errors)\n",
    "    plot(validation_errors[:,0], validation_errors[:,1], label='validation error rate', color='r')\n",
    "    ylim(0,0.2)\n",
    "    legend()\n",
    "\n",
    "class AlphaAlgExp:\n",
    "    def __init__(self, initial = 5e-2, rate = 0.998):\n",
    "        self.initial = initial\n",
    "        self.rate = rate\n",
    "    \n",
    "    def __call__(self, i, e):\n",
    "        return (self.initial * np.power(self.rate, e))\n",
    "    \n",
    "class AlphaAlgBct:\n",
    "    def __init__(self, b = 1e3, c = 2e4):\n",
    "        self.b = b\n",
    "        self.c = c\n",
    "    \n",
    "    def __call__(self, i, e):\n",
    "        return (self.b / (self.c + i))\n",
    "    \n",
    "class AlphaAlgConst:\n",
    "    def __init__(self, constant = 1e-2):\n",
    "        self.constant = constant\n",
    "        \n",
    "    def __call__(self, i, e):\n",
    "        return self.constant\n",
    "    \n",
    "    \n",
    "class MomentumAlgConst:\n",
    "    def __init__(self, constant = 0.5):\n",
    "        self.constant = constant\n",
    "        \n",
    "    def __call__(self, i):\n",
    "        return self.constant\n",
    "    \n",
    "class MomentumAlg1:\n",
    "    def __call__(self, i, e):\n",
    "        return (1. - 3. / (5. + i))\n",
    "\n",
    "class MomentumAlg2:\n",
    "    def __call__(self, i, e, limit = 0.9):\n",
    "        v = (1. - 3. / (5. + i))\n",
    "        return (v if v < limit else limit)\n",
    "    \n",
    "class MomentumAlg3:\n",
    "    def __init__(self, start = 0.5, stop = 0.9, epochs = 500):\n",
    "        self.start = start\n",
    "        self.stop = stop\n",
    "        self.epochs = epochs\n",
    "    \n",
    "    def __call__(self, i, e):\n",
    "        if e >= self.epochs:\n",
    "            return self.stop\n",
    "        else:\n",
    "            c = e / self.epochs\n",
    "            return c * self.start + (1. - c) * self.stop\n",
    "\n",
    "def SGD(net, train_stream, validation_stream, test_stream,\n",
    "       print_debug = True,\n",
    "       epochs = 3,\n",
    "       patience = 1.5,\n",
    "       alpha_alg = AlphaAlgExp(),\n",
    "       momentum_alg = MomentumAlg1(),\n",
    "       regularization_rate = 1e-3,\n",
    "       dropout = None):\n",
    "    \n",
    "    print \"Network configuration: \"\n",
    "    print \", \".join([ \"%s%s\" %(N, P.shape) for P, N in zip(net.parameters, net.parameter_names) ])\n",
    "    \n",
    "    i=0\n",
    "    e=0\n",
    "    \n",
    "    velocities = [np.zeros(P.shape) for P in net.parameters]\n",
    "    \n",
    "    best_valid_error_rate = np.inf\n",
    "    best_params = deepcopy(net.parameters)\n",
    "    best_params_epoch = 0\n",
    "    \n",
    "    alpha = None\n",
    "    \n",
    "    train_errors = []\n",
    "    train_loss = []\n",
    "    validation_errors = []\n",
    "    \n",
    "    number_of_epochs = epochs\n",
    "    patience_expansion = patience\n",
    "    \n",
    "    try:\n",
    "        while e<number_of_epochs: #This loop goes over epochs\n",
    "            e += 1\n",
    "            #First train on all data from this batch\n",
    "            for X,Y in train_stream.get_epoch_iterator(): \n",
    "                i += 1\n",
    "                L, O, gradients = net.get_cost_and_gradient(X, Y)\n",
    "                err_rate = (O.argmax(0) != Y).mean()\n",
    "                train_loss.append((i,L))\n",
    "                train_errors.append((i,err_rate))\n",
    "                if i % 100 == 0 and print_debug:\n",
    "                    print \"At minibatch %d, batch loss %f, batch error rate %f%%\" % (i, L, err_rate*100)\n",
    "                for P, V, G, N in zip(net.parameters, velocities, gradients, net.parameter_names):\n",
    "                    if N=='W' and regularization_rate:\n",
    "                        G += regularization_rate * P\n",
    "                        \n",
    "                    \n",
    "                    alpha = alpha_alg(i, e)\n",
    "                    \n",
    "                    epsilon = momentum_alg(i, e)\n",
    "                    \n",
    "                    #V = epsilon * V - (1. - epsilon) * alpha * G\n",
    "                    V = epsilon * V - alpha * G\n",
    "                    \n",
    "                    P += V\n",
    "                    \n",
    "            # After an epoch compute validation error\n",
    "            val_error_rate = compute_error_rate(net, validation_stream)\n",
    "\n",
    "            if val_error_rate < best_valid_error_rate:\n",
    "                number_of_epochs = np.maximum(number_of_epochs, e * patience_expansion+1)\n",
    "                best_valid_error_rate = val_error_rate\n",
    "                best_params = deepcopy(net.parameters)\n",
    "                best_params_epoch = e\n",
    "                validation_errors.append((i,val_error_rate))\n",
    "            if print_debug or (e % 20 == 1):\n",
    "                print \"After epoch %d: valid_err_rate: %f%% currently going ot do %d epochs\" %(\n",
    "                    e, val_error_rate, number_of_epochs)\n",
    "        print \"Finished with %d epochs (minibatch %d), with best valid error rate %f\" % (e, i, best_valid_error_rate)\n",
    "        print_stats(train_loss, train_errors, validation_errors)\n",
    "    except KeyboardInterrupt:\n",
    "        print \"Setting network parameters from after epoch %d\" %(best_params_epoch)\n",
    "        net.parameters = best_params\n",
    "        print_stats(train_loss, train_errors, validation_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network configuration: \n",
      "W(20, 1, 5, 5), b(20,), W(10, 2880), b(10, 1)\n",
      "(2000, 1, 24, 24)\n",
      "(100, 20, 24, 24)\n",
      "shapes\n",
      "(20, 1, 5, 5)\n",
      "(20, 1, 5, 5)\n",
      "(20, 1, 5, 5)\n",
      "shapes\n",
      "(20,)\n",
      "(20,)\n",
      "(20, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "non-broadcastable output operand with shape (20,) doesn't match the broadcast shape (20,20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-141-b1da87a046f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mregularization_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0malpha_alg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAlphaAlgExp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minitial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.992\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     momentum_alg = MomentumAlg3())\n\u001b[0m",
      "\u001b[0;32m<ipython-input-140-acebf3b06b74>\u001b[0m in \u001b[0;36mSGD\u001b[0;34m(net, train_stream, validation_stream, test_stream, print_debug, epochs, patience, alpha_alg, momentum_alg, regularization_rate, dropout)\u001b[0m\n\u001b[1;32m    137\u001b[0m                     \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m                     \u001b[0mP\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;31m# After an epoch compute validation error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: non-broadcastable output operand with shape (20,) doesn't match the broadcast shape (20,20)"
     ]
    }
   ],
   "source": [
    "weight_init = IsotropicGaussian(std=0.05, mean=0.0)\n",
    "net = FeedForwardNet([\n",
    "        ConvPoolLayer(image_shape=(100, 1, 28, 28), filter_shape=(20, 1, 5, 5)),\n",
    "        ReLULayer(),\n",
    "        PoolLayer(image_shape=(100, 20, 24, 24)),\n",
    "        AffineLayer(20 * 12 * 12, 10, weight_init = weight_init),\n",
    "        SoftMaxLayer()\n",
    "    ])\n",
    "\n",
    "SGD(net, mnist_train_stream, mnist_validation_stream, mnist_test_stream,\n",
    "    print_debug = True,\n",
    "    epochs = 2000,\n",
    "    regularization_rate = 0.0,\n",
    "    alpha_alg = AlphaAlgExp(initial = 5e-2, rate = 0.992),\n",
    "    momentum_alg = MomentumAlg3())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 57600)\n",
      "(20, 25)\n",
      "(20, 57600)\n",
      "(20, 57600)\n",
      "(20, 24, 24, 100)\n",
      "(100, 20, 24, 24)\n",
      "(4, 288000)\n",
      "(288000,)\n",
      "(12, 12, 100, 20)\n",
      "(100, 20, 12, 12)\n"
     ]
    }
   ],
   "source": [
    "train_stream = mnist_train_stream\n",
    "X,Y = train_stream.get_epoch_iterator().next()\n",
    "\n",
    "image_shape = (100, 1, 28, 28)\n",
    "filter_shape = (20, 1, 5, 5)\n",
    "poolsize = (2,2)\n",
    "\n",
    "\n",
    "rng = np.random\n",
    "weight_init = IsotropicGaussian(std=0.05, mean=0.0)\n",
    "W = weight_init.generate(rng, filter_shape)\n",
    "b = weight_init.generate(rng, (filter_shape[0],1))\n",
    "\n",
    "num_images, num_channels, img_w, img_h = image_shape\n",
    "num_filters, num_channels2, filter_w, filter_h = filter_shape\n",
    "\n",
    "Xr = X.reshape(image_shape)\n",
    "X_col = im2col_indices(Xr, filter_w, filter_h, stride = 1, padding=0)\n",
    "\n",
    "print X_col.shape\n",
    "\n",
    "W_col = W.reshape(num_filters, -1)\n",
    "print W_col.shape\n",
    "\n",
    "Y = W_col.dot(X_col) + b\n",
    "print Y.shape\n",
    "Y = np.maximum(Y, 0.0)\n",
    "print Y.shape\n",
    "\n",
    "        # Reshape back from 20x500 to 5x20x10x10\n",
    "        # i.e. for each of our 5 images, we have 20 results with size of 10x10\n",
    "w_out = img_w - filter_w + 1\n",
    "h_out = img_h - filter_h + 1\n",
    "\n",
    "Y = Y.reshape(num_filters, w_out, h_out, num_images)\n",
    "print Y.shape\n",
    "\n",
    "Y = Y.transpose(3, 0, 1, 2)\n",
    "print Y.shape\n",
    "\n",
    "### ^^^ was convolution\n",
    "\n",
    "Yr = Y.reshape(num_images * num_filters, 1, h_out, w_out)\n",
    "X_pool = im2col_indices(Yr, poolsize[0], poolsize[1], stride=2, padding=0)\n",
    "print X_pool.shape\n",
    "\n",
    "max_idx = np.argmax(X_pool, axis=0)\n",
    "#print max_idx.shape\n",
    "Y_pool = X_pool[max_idx, range(max_idx.size)]\n",
    "print Y_pool.shape\n",
    "\n",
    "Y_pool = Y_pool.reshape(12, 12, n, d)\n",
    "print Y_pool.shape\n",
    "\n",
    "Y_pool = Y_pool.transpose(2, 3, 0, 1)\n",
    "print Y_pool.shape\n",
    "\n",
    "### ^^^ was pooling\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# return Y, dict(X=X, Xr=Xr, X_col=X_col)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
