{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration of reinforcement learning\n",
    "\n",
    "Code written after Andrej's Karpathy description at http://karpathy.github.io/2016/05/31/rl/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym.wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:07,535] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1422\n",
      "-20.0\n",
      "CPU times: user 660 ms, sys: 40 ms, total: 700 ms\n",
      "Wall time: 699 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "env.reset()\n",
    "steps = []\n",
    "while True:\n",
    "    step_img, reward, done, extra = env.step(env.action_space.sample())\n",
    "    steps.append((step_img, reward))\n",
    "    if done:\n",
    "        break\n",
    "\n",
    "print(len(steps))\n",
    "\n",
    "print(np.sum([s[1] for s in steps]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:08,802] Making new env: Pong-v0\n",
      "INFO:gym.wrappers.monitoring:Clearing 4 monitor files from previous run (because force=True was provided)\n",
      "[2017-05-31 12:44:08,983] Clearing 4 monitor files from previous run (because force=True was provided)\n",
      "INFO:gym.monitoring.video_recorder:Starting new video recorder writing to /pio/lscratch/1/jch/private/Dropbox/work/II/summer17/nnets17/lectures/tmpgym/openaigym.video.0.4543.video000000.mp4\n",
      "[2017-05-31 12:44:08,992] Starting new video recorder writing to /pio/lscratch/1/jch/private/Dropbox/work/II/summer17/nnets17/lectures/tmpgym/openaigym.video.0.4543.video000000.mp4\n",
      "INFO:gym.wrappers.monitoring:Finished writing results. You can upload them to the scoreboard via gym.upload('/pio/lscratch/1/jch/private/Dropbox/work/II/summer17/nnets17/lectures/tmpgym')\n",
      "[2017-05-31 12:44:10,001] Finished writing results. You can upload them to the scoreboard via gym.upload('/pio/lscratch/1/jch/private/Dropbox/work/II/summer17/nnets17/lectures/tmpgym')\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"Pong-v0\")\n",
    "env = gym.wrappers.Monitor(env, \"./tmpgym/\", force=True)\n",
    "\n",
    "env.reset()\n",
    "\n",
    "steps = []\n",
    "while True:\n",
    "    step_img, reward, done, extra = env.step(env.action_space.sample())\n",
    "    steps.append((step_img, reward))\n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"./tmpgym/openaigym.video.0.4543.video000000.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_template = \"\"\"\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"%s\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "videos = []\n",
    "\n",
    "for f in glob(\"./tmpgym/*.mp4\"):\n",
    "    videos.append(video_template % (f,))\n",
    "\n",
    "HTML(''.join(videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:10,206] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "env = gym.make(\"Pong-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12057213055951277873L, 408342734L]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:10,908] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:11,331] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:11,705] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:12,441] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:12,835] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:13,260] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:13,645] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:14,046] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:14,405] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:14,768] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:15,184] Making new env: Pong-v0\n",
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 12:44:15,594] Making new env: Pong-v0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "envs = []\n",
    "\n",
    "NWORKERS = 12\n",
    "MAX_ROLLOUTS_IN_BATCH = 100\n",
    "\n",
    "for i in range(NWORKERS):\n",
    "    envs.append(gym.make(\"Pong-v0\"))\n",
    "    envs[-i].seed(i)\n",
    "\n",
    "# Add space for a monitoring env\n",
    "envs.append(None)\n",
    "\n",
    "def preproc_state(state):\n",
    "    state = state[35:195:2, ::2]\n",
    "    state = ((state[:,:,0] != 144) & (state[:,:,0] != 119)).astype('float32')\n",
    "    return state\n",
    "\n",
    "\n",
    "def discount_rewards(r, gamma=0.99):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in xrange(r.size-1, -1, -1):\n",
    "        if r[t] != 0:\n",
    "            running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r\n",
    "\n",
    "env_id = tf.placeholder('int32', (), \"env_id\")\n",
    "cur_state = tf.placeholder('float32', (80, 80), \"cur_state\")\n",
    "prev_state = tf.zeros_like(cur_state)\n",
    "\n",
    "states = tf.TensorArray('float32', size=2, dynamic_size=True, clear_after_read=False)\n",
    "states = states.write(0, prev_state)\n",
    "states = states.write(1, cur_state)\n",
    "\n",
    "done = tf.constant(False)\n",
    "\n",
    "rewards = tf.TensorArray('float32', size=0, dynamic_size=True)\n",
    "\n",
    "action_logits = tf.TensorArray('float32', size=0, dynamic_size=True)\n",
    "actions = tf.TensorArray('int32', size=0, dynamic_size=True)\n",
    "\n",
    "def net(prev_state, cur_state):\n",
    "    # return tf.constant(0.0, dtype=tf.float32)\n",
    "    with tf.variable_scope(\"network\"):\n",
    "        input_ = tf.reshape(cur_state - prev_state, (1, 80*80))\n",
    "        hidden = tf.layers.dense(input_, 200, activation=tf.nn.relu, \n",
    "                                 weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        action_logit = tf.layers.dense(hidden, 1, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "    return action_logit\n",
    "\n",
    "def while_cond(step, done, *args):\n",
    "    return tf.not_equal(done, True)\n",
    "\n",
    "def while_body(step, done, states, rewards, action_logits, actions):\n",
    "    def env_step_fun(env_id, action):\n",
    "        state, reward, done, _ = envs[env_id].step(action=2 if action else 3)\n",
    "        return preproc_state(state), np.array(reward, dtype='float32'), done\n",
    "    \n",
    "    prev_state = states.read(step)\n",
    "    cur_state = states.read(step+1)\n",
    "    \n",
    "    \n",
    "    action_logit = net(prev_state, cur_state)\n",
    "    action_logits = action_logits.write(step, action_logit)\n",
    "    \n",
    "    action = tf.cast(tf.random_uniform(()) < tf.nn.sigmoid(action_logit), tf.int32)\n",
    "    actions = actions.write(step, action)\n",
    "    \n",
    "    state, reward, done = tf.py_func(env_step_fun, [env_id, action], \n",
    "                                     [tf.float32, tf.float32, tf.bool])\n",
    "    \n",
    "    state.set_shape(cur_state.shape)\n",
    "    reward.set_shape(())\n",
    "    done.set_shape(())\n",
    "    \n",
    "    states = states.write(step+2, state)\n",
    "    rewards = rewards.write(step, reward)\n",
    "    \n",
    "    return step + 1, done, states, rewards, action_logits, actions\n",
    "    \n",
    "(steps, done, states, rewards, action_logits,actions\n",
    ") = tf.while_loop(while_cond, while_body, [0, False, states, rewards, action_logits, actions])\n",
    "\n",
    "states = states.stack()\n",
    "rewards = tf.reshape(rewards.stack(), (-1,))\n",
    "actions = tf.reshape(actions.stack(), (-1,))\n",
    "action_logits = tf.reshape(action_logits.stack(), (-1,))\n",
    "\n",
    "rewards_discounted = tf.py_func(discount_rewards, [rewards], tf.float32, stateful=False)\n",
    "rewards_discounted.set_shape(rewards.shape)\n",
    "\n",
    "rewards_mean, rewards_var = tf.nn.moments(rewards_discounted, axes=[0])\n",
    "rewards_discounted = (rewards_discounted - rewards_mean) / tf.sqrt(rewards_var)\n",
    "\n",
    "action_neg_likelihood = tf.nn.sigmoid_cross_entropy_with_logits(logits=action_logits,\n",
    "                                                                targets=tf.cast(actions, tf.float32))\n",
    "loss = tf.reduce_sum(action_neg_likelihood * rewards_discounted)\n",
    "\n",
    "accums = []\n",
    "with tf.variable_scope('accums'):\n",
    "    for var in tf.trainable_variables():\n",
    "        accums.append(tf.get_variable(var.name.split(':')[0],\n",
    "                                      var.get_shape(),\n",
    "                                      var.dtype,\n",
    "                                      tf.zeros_initializer(),\n",
    "                                      trainable=False))\n",
    "    reward_accum = tf.get_variable('rewards', (), tf.float32, tf.zeros_initializer(), trainable=False)\n",
    "        \n",
    "grads = tf.gradients(loss, tf.trainable_variables())\n",
    "\n",
    "rollout_count = tf.Variable(0, name='rollout_count', trainable=False)\n",
    "\n",
    "grad_count = tf.Variable(0, name='grad_count_count', trainable=False)\n",
    "\n",
    "grad_update_op = tf.group(tf.assign_add(rollout_count, 1),\n",
    "                          tf.assign_add(grad_count, 1),\n",
    "                          tf.assign_add(reward_accum, tf.reduce_sum(rewards)),\n",
    "                          *[tf.assign_add(a, g, use_locking=True) \n",
    "                            for a,g in zip(accums, grads)])\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "trainer = tf.train.AdamOptimizer()\n",
    "train_op = trainer.apply_gradients(zip([a / tf.cast(grad_count, 'float32') for a in accums], \n",
    "                                       tf.trainable_variables()), global_step)\n",
    "\n",
    "reset_accums_op = tf.group(tf.assign(grad_count, 0),\n",
    "                           tf.assign(reward_accum, 0.0),\n",
    "                           *[tf.assign(a, tf.zeros_like(a)) for a in accums])\n",
    "\n",
    "#train_op = tf.group(reset_accums_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "int_placeholder = tf.placeholder('int32', (), 'int_placeholder')\n",
    "work_queue = tf.FIFOQueue(MAX_ROLLOUTS_IN_BATCH, [tf.int32], shapes=[()])\n",
    "work_enqueue = work_queue.enqueue(int_placeholder)\n",
    "work_deque_one = work_queue.dequeue()\n",
    "work_deque_all = work_queue.dequeue_many(int_placeholder)\n",
    "\n",
    "done_queue = tf.FIFOQueue(MAX_ROLLOUTS_IN_BATCH, [tf.int32], shapes=[()])\n",
    "done_enqueue = done_queue.enqueue(int_placeholder)\n",
    "done_deque_one = done_queue.dequeue()\n",
    "done_deque_all = done_queue.dequeue_many(int_placeholder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-c4ac887da2a7>:1: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-c4ac887da2a7>:1: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "[2017-05-31 14:08:38,645] From <ipython-input-17-c4ac887da2a7>:1: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver(tf.global_variables(), keep_checkpoint_every_n_hours=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def env_simulator(env_id_v):\n",
    "    # print (\"Starting env_id %d\" %(env_id_v,))\n",
    "    while True:\n",
    "        job = sess.run(work_deque_one)\n",
    "        # print (\"Running env_id %d job %d\" %(env_id_v, job,))\n",
    "        sess.run([grad_update_op], \n",
    "                 feed_dict={cur_state: preproc_state(envs[env_id_v].reset()),\n",
    "                            env_id: env_id_v})\n",
    "        sess.run([done_enqueue], feed_dict={int_placeholder: job})\n",
    "    \n",
    "threads = []\n",
    "for i in range(NWORKERS):\n",
    "    thread = threading.Thread(target=env_simulator, args=(i,))\n",
    "    threads.append(thread)\n",
    "    thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./tmpgym/tf_save-320'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, \"./tmpgym/tf_save\", global_step=global_step_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gcv: 10\n",
      "After 321 steps (3230 rollouts) the mean reward is -8.400000\n",
      "gcv: 10\n",
      "After 322 steps (3240 rollouts) the mean reward is -5.700000\n",
      "gcv: 10\n",
      "After 323 steps (3250 rollouts) the mean reward is -6.800000\n",
      "gcv: 10\n",
      "After 324 steps (3260 rollouts) the mean reward is -4.500000\n",
      "gcv: 10\n",
      "After 325 steps (3270 rollouts) the mean reward is -9.400000\n",
      "gcv: 10\n",
      "After 326 steps (3280 rollouts) the mean reward is -5.200000\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "# Reset the accumulators, which can be non-zero after a resumed training\n",
    "sess.run(reset_accums_op)\n",
    "\n",
    "while True:\n",
    "    # accumulate gradients over a minibatch of inputs\n",
    "    for i in range(BATCH_SIZE):\n",
    "        sess.run(work_enqueue, feed_dict={int_placeholder: i})\n",
    "    sess.run(done_deque_all, feed_dict={int_placeholder: BATCH_SIZE})\n",
    "    \n",
    "    # read the statistics\n",
    "    grad_count_v, reward_accum_v, global_step_v, rollout_count_v = sess.run(\n",
    "        [grad_count, reward_accum, global_step, rollout_count])\n",
    "\n",
    "    assert grad_count_v == BATCH_SIZE\n",
    "    \n",
    "    print (\"After %d steps (%d rollouts) the mean reward is %f\" % \n",
    "           (global_step_v, rollout_count_v, reward_accum_v / grad_count_v))\n",
    "    \n",
    "    sess.run(train_op)\n",
    "    sess.run(reset_accums_op)\n",
    "    \n",
    "    if (global_step_v % 10) == 0:\n",
    "        saver.save(sess, \"./tmpgym/tf_save\", global_step=global_step_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gym.envs.registration:Making new env: Pong-v0\n",
      "[2017-05-31 14:16:53,122] Making new env: Pong-v0\n",
      "INFO:gym.wrappers.monitoring:Creating monitor directory ./tmpgym/videos\n",
      "[2017-05-31 14:16:53,423] Creating monitor directory ./tmpgym/videos\n",
      "INFO:gym.monitoring.video_recorder:Starting new video recorder writing to /pio/lscratch/1/jch/private/Dropbox/work/II/summer17/nnets17/lectures/tmpgym/videos/openaigym.video.1.4543.video000000.mp4\n",
      "[2017-05-31 14:16:53,433] Starting new video recorder writing to /pio/lscratch/1/jch/private/Dropbox/work/II/summer17/nnets17/lectures/tmpgym/videos/openaigym.video.1.4543.video000000.mp4\n",
      "INFO:gym.wrappers.monitoring:Finished writing results. You can upload them to the scoreboard via gym.upload('/pio/lscratch/1/jch/private/Dropbox/work/II/summer17/nnets17/lectures/tmpgym/videos')\n",
      "[2017-05-31 14:17:00,706] Finished writing results. You can upload them to the scoreboard via gym.upload('/pio/lscratch/1/jch/private/Dropbox/work/II/summer17/nnets17/lectures/tmpgym/videos')\n"
     ]
    }
   ],
   "source": [
    "# Play a game and recodr a video!\n",
    "\n",
    "envs[-1] = gym.wrappers.Monitor(gym.make(\"Pong-v0\"), \"./tmpgym/videos\", force=True)\n",
    "env_monitor = len(envs) - 1\n",
    "sess.run(rewards, {cur_state: preproc_state(envs[env_monitor].reset()),\n",
    "                   env_id: env_monitor})\n",
    "envs[-1].close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "./tmpgym/gym1/openaigym.video.0.4543.video000000.mp4 <br>\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"./tmpgym/gym1/openaigym.video.0.4543.video000000.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video><br>\n",
       "\n",
       "./tmpgym/videos/openaigym.video.1.4543.video000000.mp4 <br>\n",
       "<video width=\"320\" height=\"240\" controls>\n",
       "  <source src=\"./tmpgym/videos/openaigym.video.1.4543.video000000.mp4\" type=\"video/mp4\">\n",
       "  Your browser does not support the video tag.\n",
       "</video><br>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_template = \"\"\"\n",
    "%s <br>\n",
    "<video width=\"320\" height=\"240\" controls>\n",
    "  <source src=\"%s\" type=\"video/mp4\">\n",
    "  Your browser does not support the video tag.\n",
    "</video><br>\n",
    "\"\"\"\n",
    "\n",
    "videos = []\n",
    "\n",
    "for f in glob(\"./tmpgym/*/*.mp4\"):\n",
    "    videos.append(video_template % (f,f))\n",
    "\n",
    "HTML(''.join(videos))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
