{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "num_hidden1 = 500 # 1st layer\n",
    "num_hidden2 = 500 # 2nd layer\n",
    "\n",
    "log_dir = \"./log\"\n",
    "\n",
    "max_features = 500 # Tfidf features\n",
    "\n",
    "max_steps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blt_dataset import CATEGORIES, export_events_data\n",
    "\n",
    "events = export_events_data(\"../datasets/events1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How to represent cyclic input?\n",
    "#   For example, how to represent that 31.21 is close to 01.01? Or we should leave it to NN?\n",
    "# Discretize!\n",
    "# span in days from the beginning of each month\n",
    "# span in hours from daily hour, whether event starts in the morning or notwis\n",
    "# add dropout\n",
    "# add tfidf from title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_texts = [e['description'] for e in events]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk.download() # needs 'punkt' package\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# Create TF-IDF of texts\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer, stop_words=None, max_features=max_features)\n",
    "sparse_tfidf_texts = tfidf.fit_transform(raw_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193847.0\n",
      "172139.0\n",
      "82787.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def flattener(left, right):\n",
    "    try:\n",
    "        res = reduce(flattener, right, left)\n",
    "    except TypeError:\n",
    "        left.append(right)\n",
    "        res = left\n",
    "    return res\n",
    "\n",
    "def flatten(seq):\n",
    "    return reduce(flattener, seq, [])\n",
    "\n",
    "attrs_bool = np.nan_to_num(np.array([e['attrs_bool'] for e in events], dtype = np.float32))\n",
    "attrs_bool.shape\n",
    "print(np.sum(attrs_bool))\n",
    "\n",
    "# attrs_scale01 = np.array([np.concatenate(e['attrs_scale01']).tolist() for e in events], dtype = np.float32)\n",
    "# [np.concatenate(e['attrs_scale01']).tolist() for e in events]\n",
    "attrs_scale01 = np.array([flatten(e['attrs_scale01']) for e in events], dtype = np.float32)\n",
    "attrs_scale01 /= np.max(attrs_scale01, axis = 0)\n",
    "attrs_scale01.shape\n",
    "print(np.sum(attrs_scale01))\n",
    "\n",
    "attrs_logscale01 = np.ma.log(np.array([flatten(e['attrs_logscale01']) for e in events], dtype = np.float32)).filled(0)\n",
    "attrs_logscale01 /= np.max(attrs_logscale01, axis = 0)\n",
    "attrs_logscale01.shape\n",
    "print(np.sum(attrs_logscale01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107838, 519)\n",
      "(107838,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "960172.26506490353"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.hstack([\n",
    "    sparse_tfidf_texts.todense(),\n",
    "    attrs_bool,\n",
    "    attrs_scale01,\n",
    "    attrs_logscale01,\n",
    "])\n",
    "print(features.shape)\n",
    "\n",
    "target = np.array([e['category'] for e in events])\n",
    "print(target.shape)\n",
    "\n",
    "np.sum(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples, num_features = features.shape\n",
    "\n",
    "train_indices = np.random.choice(num_samples, int(0.8*num_samples), replace=False)\n",
    "test_indices = np.array(list(set(range(num_samples)) - set(train_indices)))\n",
    "\n",
    "features_train = features[train_indices]\n",
    "features_test = features[test_indices]\n",
    "\n",
    "target_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])\n",
    "target_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation # 500. Train Loss (Test Loss): 2.624 (2.625). Train Acc (Test Acc): 0.185 (0.178)\n",
      "Generation # 1000. Train Loss (Test Loss): 2.606 (2.610). Train Acc (Test Acc): 0.245 (0.198)\n",
      "Generation # 1500. Train Loss (Test Loss): 2.599 (2.596). Train Acc (Test Acc): 0.205 (0.199)\n",
      "Generation # 2000. Train Loss (Test Loss): 2.592 (2.583). Train Acc (Test Acc): 0.170 (0.200)\n",
      "Generation # 2500. Train Loss (Test Loss): 2.571 (2.570). Train Acc (Test Acc): 0.220 (0.200)\n",
      "Generation # 3000. Train Loss (Test Loss): 2.559 (2.556). Train Acc (Test Acc): 0.220 (0.200)\n",
      "Generation # 3500. Train Loss (Test Loss): 2.538 (2.543). Train Acc (Test Acc): 0.225 (0.201)\n",
      "Generation # 4000. Train Loss (Test Loss): 2.536 (2.530). Train Acc (Test Acc): 0.200 (0.201)\n",
      "Generation # 4500. Train Loss (Test Loss): 2.523 (2.517). Train Acc (Test Acc): 0.220 (0.203)\n",
      "Generation # 5000. Train Loss (Test Loss): 2.494 (2.504). Train Acc (Test Acc): 0.255 (0.203)\n",
      "Generation # 5500. Train Loss (Test Loss): 2.491 (2.491). Train Acc (Test Acc): 0.185 (0.203)\n",
      "Generation # 6000. Train Loss (Test Loss): 2.487 (2.478). Train Acc (Test Acc): 0.170 (0.202)\n",
      "Generation # 6500. Train Loss (Test Loss): 2.473 (2.465). Train Acc (Test Acc): 0.205 (0.203)\n",
      "Generation # 7000. Train Loss (Test Loss): 2.422 (2.453). Train Acc (Test Acc): 0.275 (0.203)\n",
      "Generation # 7500. Train Loss (Test Loss): 2.446 (2.441). Train Acc (Test Acc): 0.165 (0.204)\n",
      "Generation # 8000. Train Loss (Test Loss): 2.458 (2.429). Train Acc (Test Acc): 0.205 (0.205)\n",
      "Generation # 8500. Train Loss (Test Loss): 2.434 (2.417). Train Acc (Test Acc): 0.240 (0.204)\n",
      "Generation # 9000. Train Loss (Test Loss): 2.424 (2.406). Train Acc (Test Acc): 0.155 (0.204)\n",
      "Generation # 9500. Train Loss (Test Loss): 2.358 (2.396). Train Acc (Test Acc): 0.220 (0.204)\n",
      "Generation # 10000. Train Loss (Test Loss): 2.354 (2.386). Train Acc (Test Acc): 0.245 (0.205)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()   \n",
    "\n",
    "NUM_CLASSES = len(CATEGORIES)\n",
    "\n",
    "current_log_dir = os.path.join(log_dir, datetime.now().strftime(\"%Y-%m-%d-%H_%M_%S\"))\n",
    "tf.gfile.MakeDirs(current_log_dir)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x_data = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\n",
    "    y_target = tf.placeholder(tf.int32, shape=(None))\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    \n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights1 = tf.Variable(\n",
    "            tf.truncated_normal([num_features, num_hidden1],\n",
    "                            stddev=1.0 / math.sqrt(float(num_features))),\n",
    "                            name='weights')\n",
    "        biases1 = tf.Variable(tf.zeros([num_hidden1]),\n",
    "                             name='biases')\n",
    "        hidden_relu1 = tf.nn.relu(tf.matmul(x_data, weights1) + biases1)\n",
    "        hidden1 = tf.nn.dropout(hidden_relu1, keep_prob, name = 'dropout')\n",
    "\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights2 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden1, num_hidden2],\n",
    "                            stddev=1.0 / math.sqrt(float(num_hidden1))),\n",
    "                            name='weights')\n",
    "        biases2 = tf.Variable(tf.zeros([num_hidden2]),\n",
    "                             name='biases')\n",
    "        hidden_relu2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "        hidden2 = tf.nn.dropout(hidden_relu2, keep_prob, name = 'dropout')\n",
    "\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights_sm = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden2, NUM_CLASSES],\n",
    "                            stddev=1.0 / math.sqrt(float(num_hidden2))),\n",
    "                            name='weights')\n",
    "        biases_sm = tf.Variable(tf.zeros([NUM_CLASSES]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2, weights_sm) + biases_sm\n",
    "\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "        labels=y_target,\n",
    "        logits=logits,\n",
    "        name='xentropy')\n",
    "    loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    # optimizer = tf.train.AdamOptimizer(0.00025)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.00025)\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    prediction = tf.nn.in_top_k(logits, y_target, 1)\n",
    "    predictions_correct = tf.cast(prediction, tf.float32)\n",
    "    accuracy = tf.reduce_mean(predictions_correct)\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    \n",
    "    summary = tf.summary.merge_all()\n",
    "    \n",
    "    # Create a saver for writing training checkpoints.\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "    summary_writer = tf.summary.FileWriter(current_log_dir, sess.graph)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for i in range(max_steps):\n",
    "        rand_index = np.random.choice(features_train.shape[0], size=batch_size)\n",
    "        rand_x = features_train[rand_index]\n",
    "        rand_y = np.transpose([target_train[rand_index]]).ravel()\n",
    "        feed_dict = {\n",
    "            x_data: rand_x,\n",
    "            y_target: rand_y,\n",
    "            keep_prob: 0.5,\n",
    "        }\n",
    "        \n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "\n",
    "        # Only record loss and accuracy every 100 generations\n",
    "        if (i+1)%100==0:\n",
    "            feed_dict_train = {\n",
    "                x_data: rand_x,\n",
    "                y_target: rand_y,\n",
    "                keep_prob: 1.0,\n",
    "            }\n",
    "\n",
    "            feed_dict_test = {\n",
    "                x_data: features_test,\n",
    "                y_target: np.transpose([target_test]).ravel(),\n",
    "                keep_prob: 1.0,\n",
    "            }\n",
    "\n",
    "            train_loss_temp = sess.run(loss, feed_dict=feed_dict_train)\n",
    "\n",
    "            test_loss_temp = sess.run(loss, feed_dict=feed_dict_test)\n",
    "\n",
    "            train_acc_temp = sess.run(accuracy, feed_dict=feed_dict_train)\n",
    "\n",
    "            test_acc_temp = sess.run(accuracy, feed_dict=feed_dict_test)\n",
    "            \n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "            summary_writer.add_summary(summary_str, i)\n",
    "            summary_writer.flush()\n",
    "\n",
    "        if (i+1)%500==0:\n",
    "            acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp * 100, test_acc_temp * 100]\n",
    "            acc_and_loss = [np.round(x,3) for x in acc_and_loss]\n",
    "            print('Generation # {}. Train Loss (Test Loss): {:.3f} ({:.3f}). Train Acc (Test Acc): {:.3f} ({:.3f})'.format(*acc_and_loss))\n",
    "            \n",
    "            checkpoint_file = os.path.join(current_log_dir, 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_file, global_step=i)\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
