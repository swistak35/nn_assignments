{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 200\n",
    "num_hidden1 = 500 # 1st layer\n",
    "num_hidden2 = 500 # 2nd layer\n",
    "\n",
    "log_dir = \"./log\"\n",
    "\n",
    "max_features = 500 # Tfidf features\n",
    "\n",
    "min_steps = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blt_dataset import CATEGORIES, export_events_data\n",
    "\n",
    "events = export_events_data(\"../datasets/events1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How to represent cyclic input?\n",
    "#   For example, how to represent that 31.12 is close to 01.01? Or we should leave it to NN?\n",
    "# Discretize!\n",
    "# span in days from the beginning of each month\n",
    "# span in hours from daily hour, whether event starts in the morning or notwis\n",
    "# add tfidf from title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_desc_texts = [e['description'] for e in events]\n",
    "raw_title_texts = [e['title'] for e in events]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk.download() # needs 'punkt' package\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# Create TF-IDF of texts\n",
    "tfidf_desc = TfidfVectorizer(tokenizer=tokenizer, stop_words=None, max_features=max_features)\n",
    "sparse_tfidf_desc = tfidf_desc.fit_transform(raw_desc_texts)\n",
    "\n",
    "tfidf_title = TfidfVectorizer(tokenizer=tokenizer, stop_words=None, max_features=max_features)\n",
    "sparse_tfidf_title = tfidf_title.fit_transform(raw_title_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193847.0\n",
      "172139.0\n",
      "82787.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "attrs_bool = np.nan_to_num(np.array([e['attrs_bool'] for e in events], dtype = np.float32))\n",
    "attrs_bool.shape\n",
    "print(np.sum(attrs_bool))\n",
    "\n",
    "attrs_scale01 = np.array([e['attrs_scale01'] for e in events], dtype = np.float32)\n",
    "attrs_scale01 /= np.max(attrs_scale01, axis = 0)\n",
    "attrs_scale01.shape\n",
    "print(np.sum(attrs_scale01))\n",
    "\n",
    "attrs_logscale01 = np.ma.log(np.array([e['attrs_logscale01'] for e in events], dtype = np.float32)).filled(0)\n",
    "attrs_logscale01 /= np.max(attrs_logscale01, axis = 0)\n",
    "attrs_logscale01.shape\n",
    "print(np.sum(attrs_logscale01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107838, 1019)\n",
      "(107838,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1091126.0498753795"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.hstack([\n",
    "    sparse_tfidf_desc.todense(),\n",
    "    sparse_tfidf_title.todense(),\n",
    "    attrs_bool,\n",
    "    attrs_scale01,\n",
    "    attrs_logscale01,\n",
    "])\n",
    "print(features.shape)\n",
    "\n",
    "target = np.array([e['category'] for e in events])\n",
    "print(target.shape)\n",
    "\n",
    "np.sum(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples, num_features = features.shape\n",
    "\n",
    "train_indices = np.random.choice(num_samples, int(0.8*num_samples), replace=False)\n",
    "test_indices = np.array(list(set(range(num_samples)) - set(train_indices)))\n",
    "\n",
    "features_train = features[train_indices]\n",
    "features_test = features[test_indices]\n",
    "\n",
    "target_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])\n",
    "target_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy improved, now steps count is 20000\n",
      "Generation # 500. Train Loss (Test Loss): 2.633 (2.631). Train Acc (Test Acc): 9.000 (9.737)\n",
      "Generation # 1000. Train Loss (Test Loss): 2.621 (2.619). Train Acc (Test Acc): 16.500 (15.996)\n",
      "Generation # 1500. Train Loss (Test Loss): 2.612 (2.608). Train Acc (Test Acc): 14.500 (19.891)\n",
      "Generation # 2000. Train Loss (Test Loss): 2.597 (2.596). Train Acc (Test Acc): 24.000 (20.391)\n",
      "Generation # 2500. Train Loss (Test Loss): 2.581 (2.585). Train Acc (Test Acc): 21.500 (20.530)\n",
      "Generation # 3000. Train Loss (Test Loss): 2.578 (2.574). Train Acc (Test Acc): 22.500 (20.498)\n",
      "Generation # 3500. Train Loss (Test Loss): 2.567 (2.562). Train Acc (Test Acc): 21.000 (20.605)\n",
      "Generation # 4000. Train Loss (Test Loss): 2.559 (2.551). Train Acc (Test Acc): 20.000 (20.591)\n",
      "Generation # 4500. Train Loss (Test Loss): 2.539 (2.540). Train Acc (Test Acc): 20.000 (20.707)\n",
      "Generation # 5000. Train Loss (Test Loss): 2.534 (2.529). Train Acc (Test Acc): 19.000 (20.758)\n",
      "Generation # 5500. Train Loss (Test Loss): 2.527 (2.518). Train Acc (Test Acc): 19.500 (20.753)\n",
      "Generation # 6000. Train Loss (Test Loss): 2.530 (2.507). Train Acc (Test Acc): 18.000 (20.836)\n",
      "Generation # 6500. Train Loss (Test Loss): 2.501 (2.496). Train Acc (Test Acc): 18.500 (20.869)\n",
      "Generation # 7000. Train Loss (Test Loss): 2.468 (2.485). Train Acc (Test Acc): 24.000 (20.952)\n",
      "Generation # 7500. Train Loss (Test Loss): 2.480 (2.474). Train Acc (Test Acc): 20.500 (21.078)\n",
      "Generation # 8000. Train Loss (Test Loss): 2.432 (2.462). Train Acc (Test Acc): 25.500 (21.129)\n",
      "Generation # 8500. Train Loss (Test Loss): 2.428 (2.451). Train Acc (Test Acc): 24.000 (21.156)\n",
      "Accuracy improved, now steps count is 20000\n",
      "Accuracy improved, now steps count is 20000\n",
      "Generation # 9000. Train Loss (Test Loss): 2.442 (2.441). Train Acc (Test Acc): 17.500 (21.217)\n",
      "Accuracy improved, now steps count is 20000\n",
      "Generation # 9500. Train Loss (Test Loss): 2.401 (2.430). Train Acc (Test Acc): 27.500 (21.282)\n",
      "Accuracy improved, now steps count is 20000\n",
      "Accuracy improved, now steps count is 20000\n",
      "Generation # 10000. Train Loss (Test Loss): 2.398 (2.419). Train Acc (Test Acc): 23.500 (21.300)\n",
      "Generation # 10500. Train Loss (Test Loss): 2.429 (2.409). Train Acc (Test Acc): 20.500 (21.342)\n",
      "Generation # 11000. Train Loss (Test Loss): 2.367 (2.399). Train Acc (Test Acc): 20.500 (21.370)\n",
      "Accuracy improved, now steps count is 21998\n",
      "Generation # 11500. Train Loss (Test Loss): 2.393 (2.389). Train Acc (Test Acc): 23.000 (21.379)\n",
      "Accuracy improved, now steps count is 22998\n",
      "Accuracy improved, now steps count is 23198\n",
      "Generation # 12000. Train Loss (Test Loss): 2.367 (2.380). Train Acc (Test Acc): 27.500 (21.351)\n",
      "Generation # 12500. Train Loss (Test Loss): 2.434 (2.372). Train Acc (Test Acc): 18.000 (21.356)\n",
      "Generation # 13000. Train Loss (Test Loss): 2.341 (2.364). Train Acc (Test Acc): 21.000 (21.356)\n",
      "Generation # 13500. Train Loss (Test Loss): 2.402 (2.356). Train Acc (Test Acc): 16.000 (21.351)\n",
      "Generation # 14000. Train Loss (Test Loss): 2.353 (2.349). Train Acc (Test Acc): 21.000 (21.319)\n",
      "Generation # 14500. Train Loss (Test Loss): 2.351 (2.343). Train Acc (Test Acc): 21.000 (21.314)\n",
      "Generation # 15000. Train Loss (Test Loss): 2.337 (2.337). Train Acc (Test Acc): 19.500 (21.305)\n",
      "Generation # 15500. Train Loss (Test Loss): 2.281 (2.332). Train Acc (Test Acc): 23.000 (21.300)\n",
      "Generation # 16000. Train Loss (Test Loss): 2.356 (2.327). Train Acc (Test Acc): 21.500 (21.300)\n",
      "Generation # 16500. Train Loss (Test Loss): 2.298 (2.322). Train Acc (Test Acc): 21.000 (21.314)\n",
      "Generation # 17000. Train Loss (Test Loss): 2.349 (2.318). Train Acc (Test Acc): 20.500 (21.282)\n",
      "Generation # 17500. Train Loss (Test Loss): 2.311 (2.315). Train Acc (Test Acc): 24.000 (21.295)\n",
      "Generation # 18000. Train Loss (Test Loss): 2.409 (2.312). Train Acc (Test Acc): 16.000 (21.295)\n",
      "Generation # 18500. Train Loss (Test Loss): 2.274 (2.309). Train Acc (Test Acc): 25.500 (21.286)\n",
      "Generation # 19000. Train Loss (Test Loss): 2.320 (2.306). Train Acc (Test Acc): 19.500 (21.286)\n",
      "Generation # 19500. Train Loss (Test Loss): 2.273 (2.303). Train Acc (Test Acc): 20.500 (21.268)\n",
      "Generation # 20000. Train Loss (Test Loss): 2.292 (2.301). Train Acc (Test Acc): 21.500 (21.272)\n",
      "Generation # 20500. Train Loss (Test Loss): 2.268 (2.299). Train Acc (Test Acc): 23.500 (21.305)\n",
      "Generation # 21000. Train Loss (Test Loss): 2.358 (2.297). Train Acc (Test Acc): 20.500 (21.295)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()   \n",
    "\n",
    "num_classes = len(CATEGORIES)\n",
    "\n",
    "current_log_dir = os.path.join(log_dir, datetime.now().strftime(\"%Y-%m-%d-%H_%M_%S\"))\n",
    "tf.gfile.MakeDirs(current_log_dir)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x_data = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\n",
    "    y_target = tf.placeholder(tf.int32, shape=(None))\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    \n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights1 = tf.Variable(\n",
    "            tf.truncated_normal([num_features, num_hidden1],\n",
    "                            stddev=1.0 / math.sqrt(float(num_features))),\n",
    "                            name='weights')\n",
    "        biases1 = tf.Variable(tf.zeros([num_hidden1]),\n",
    "                             name='biases')\n",
    "        hidden_relu1 = tf.nn.relu(tf.matmul(x_data, weights1) + biases1)\n",
    "        hidden1 = tf.nn.dropout(hidden_relu1, keep_prob, name = 'dropout')\n",
    "\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights2 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden1, num_hidden2],\n",
    "                            stddev=1.0 / math.sqrt(float(num_hidden1))),\n",
    "                            name='weights')\n",
    "        biases2 = tf.Variable(tf.zeros([num_hidden2]),\n",
    "                             name='biases')\n",
    "        hidden_relu2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "        hidden2 = tf.nn.dropout(hidden_relu2, keep_prob, name = 'dropout')\n",
    "\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights_sm = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden2, num_classes],\n",
    "                            stddev=1.0 / math.sqrt(float(num_hidden2))),\n",
    "                            name='weights')\n",
    "        biases_sm = tf.Variable(tf.zeros([num_classes]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2, weights_sm) + biases_sm\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y_target,\n",
    "            logits=logits,\n",
    "            name='xentropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        # optimizer = tf.train.AdamOptimizer(0.00025)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.00025)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    with tf.name_scope('accuracy1'):\n",
    "        prediction1 = tf.nn.in_top_k(logits, y_target, 1)\n",
    "        accuracy1 = tf.reduce_mean(tf.cast(prediction1, tf.float32))\n",
    "        tf.summary.scalar('accuracy1', accuracy1)\n",
    "    \n",
    "    with tf.name_scope('accuracy2'):\n",
    "        prediction2 = tf.nn.in_top_k(logits, y_target, 2)\n",
    "        accuracy2 = tf.reduce_mean(tf.cast(prediction2, tf.float32))\n",
    "        tf.summary.scalar('accuracy2', accuracy2)\n",
    "    \n",
    "    summary = tf.summary.merge_all()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(current_log_dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(current_log_dir + '/test')\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    best_result = 0.0\n",
    "    max_steps = min_steps\n",
    "    i = 0\n",
    "    while i < max_steps:\n",
    "        rand_index = np.random.choice(features_train.shape[0], size=batch_size)\n",
    "        rand_x = features_train[rand_index]\n",
    "        rand_y = np.transpose([target_train[rand_index]]).ravel()\n",
    "        feed_dict = {\n",
    "            x_data: rand_x,\n",
    "            y_target: rand_y,\n",
    "            keep_prob: 0.5,\n",
    "        }\n",
    "        \n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "\n",
    "        # Only record loss and accuracy every 100 generations\n",
    "        if (i+1)%100==0:\n",
    "            feed_dict_train = {\n",
    "                x_data: rand_x,\n",
    "                y_target: rand_y,\n",
    "                keep_prob: 1.0,\n",
    "            }\n",
    "\n",
    "            feed_dict_test = {\n",
    "                x_data: features_test,\n",
    "                y_target: np.transpose([target_test]).ravel(),\n",
    "                keep_prob: 1.0,\n",
    "            }\n",
    "\n",
    "            train_loss_temp = sess.run(loss, feed_dict=feed_dict_train)\n",
    "            train_acc_temp = sess.run(accuracy1, feed_dict=feed_dict_train)\n",
    "            train_acc2_temp = sess.run(accuracy2, feed_dict=feed_dict_train)\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict_train)\n",
    "            train_writer.add_summary(summary_str, i)\n",
    "            train_writer.flush()\n",
    "\n",
    "            test_loss_temp = sess.run(loss, feed_dict=feed_dict_test)\n",
    "            test_acc_temp = sess.run(accuracy1, feed_dict=feed_dict_test)\n",
    "            test_acc2_temp = sess.run(accuracy2, feed_dict=feed_dict_train)\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict_test)\n",
    "            test_writer.add_summary(summary_str, i)\n",
    "            test_writer.flush()\n",
    "\n",
    "        if (i+1)%500==0:\n",
    "            acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp * 100, test_acc_temp * 100]\n",
    "            acc_and_loss = [np.round(x,3) for x in acc_and_loss]\n",
    "            print('Generation # {}. Train Loss (Test Loss): {:.3f} ({:.3f}). Train Acc (Test Acc): {:.3f} ({:.3f})'.format(*acc_and_loss))\n",
    "            \n",
    "            saver.save(sess, current_log_dir + '/model.ckpt', global_step=i)\n",
    "            \n",
    "        if test_acc_temp > best_result:\n",
    "            best_result = test_acc_temp\n",
    "            max_steps = np.max([max_steps, i * 2.0])\n",
    "            print(\"Accuracy improved, now steps count is %d\" % max_steps)\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
