{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "num_hidden1 = 2000 # 1st layer\n",
    "num_hidden2 = 2000 # 2nd layer\n",
    "\n",
    "log_dir = \"./log\"\n",
    "\n",
    "max_features = 500 # Tfidf features\n",
    "\n",
    "min_steps = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from blt_dataset import CATEGORIES, export_events_data\n",
    "\n",
    "events = export_events_data(\"../datasets/events1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How to represent cyclic input?\n",
    "#   For example, how to represent that 31.12 is close to 01.01? Or we should leave it to NN?\n",
    "# Discretize!\n",
    "# span in days from the beginning of each month\n",
    "# span in hours from daily hour, whether event starts in the morning or notwis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_desc_texts = [e['description'] for e in events]\n",
    "raw_title_texts = [e['title'] for e in events]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# nltk.download() # needs 'punkt' package\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return words\n",
    "\n",
    "# Create TF-IDF of texts\n",
    "tfidf_desc = TfidfVectorizer(tokenizer=tokenizer, stop_words=None, max_features=max_features)\n",
    "sparse_tfidf_desc = tfidf_desc.fit_transform(raw_desc_texts)\n",
    "\n",
    "tfidf_title = TfidfVectorizer(tokenizer=tokenizer, stop_words=None, max_features=max_features)\n",
    "sparse_tfidf_title = tfidf_title.fit_transform(raw_title_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "193847.0\n",
      "172139.0\n",
      "82787.9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "attrs_bool = np.nan_to_num(np.array([e['attrs_bool'] for e in events], dtype = np.float32))\n",
    "attrs_bool.shape\n",
    "print(np.sum(attrs_bool))\n",
    "\n",
    "attrs_scale01 = np.array([e['attrs_scale01'] for e in events], dtype = np.float32)\n",
    "attrs_scale01 /= np.max(attrs_scale01, axis = 0)\n",
    "attrs_scale01.shape\n",
    "print(np.sum(attrs_scale01))\n",
    "\n",
    "attrs_logscale01 = np.ma.log(np.array([e['attrs_logscale01'] for e in events], dtype = np.float32)).filled(0)\n",
    "attrs_logscale01 /= np.max(attrs_logscale01, axis = 0)\n",
    "attrs_logscale01.shape\n",
    "print(np.sum(attrs_logscale01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(107838, 1019)\n",
      "(107838,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1091126.0498753795"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = np.hstack([\n",
    "    sparse_tfidf_desc.todense(),\n",
    "    sparse_tfidf_title.todense(),\n",
    "    attrs_bool,\n",
    "    attrs_scale01,\n",
    "    attrs_logscale01,\n",
    "])\n",
    "print(features.shape)\n",
    "\n",
    "target = np.array([e['category'] for e in events])\n",
    "print(target.shape)\n",
    "\n",
    "np.sum(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_samples, num_features = features.shape\n",
    "\n",
    "train_indices = np.random.choice(num_samples, int(0.8*num_samples), replace=False)\n",
    "test_indices = np.array(list(set(range(num_samples)) - set(train_indices)))\n",
    "\n",
    "features_train = features[train_indices]\n",
    "features_test = features[test_indices]\n",
    "\n",
    "target_train = np.array([x for ix, x in enumerate(target) if ix in train_indices])\n",
    "target_test = np.array([x for ix, x in enumerate(target) if ix in test_indices])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation # 500. Train Loss (Test Loss): 2.621 (2.620). Train Acc (Test Acc): 21.000 (20.530)\n",
      "Accuracy improved, now steps count is 20000\n",
      "Generation # 1000. Train Loss (Test Loss): 2.600 (2.599). Train Acc (Test Acc): 21.100 (20.707)\n",
      "Accuracy improved, now steps count is 20000\n",
      "Generation # 1500. Train Loss (Test Loss): 2.580 (2.580). Train Acc (Test Acc): 23.300 (20.693)\n",
      "Generation # 2000. Train Loss (Test Loss): 2.564 (2.560). Train Acc (Test Acc): 19.800 (20.568)\n",
      "Generation # 2500. Train Loss (Test Loss): 2.542 (2.542). Train Acc (Test Acc): 21.800 (20.679)\n",
      "Generation # 3000. Train Loss (Test Loss): 2.514 (2.523). Train Acc (Test Acc): 21.800 (20.734)\n",
      "Accuracy improved, now steps count is 20000\n",
      "Generation # 3500. Train Loss (Test Loss): 2.503 (2.505). Train Acc (Test Acc): 20.700 (20.679)\n",
      "Generation # 4000. Train Loss (Test Loss): 2.474 (2.488). Train Acc (Test Acc): 22.700 (20.688)\n",
      "Generation # 4500. Train Loss (Test Loss): 2.474 (2.471). Train Acc (Test Acc): 22.800 (20.605)\n",
      "Generation # 5000. Train Loss (Test Loss): 2.446 (2.455). Train Acc (Test Acc): 23.100 (20.642)\n",
      "Generation # 5500. Train Loss (Test Loss): 2.450 (2.439). Train Acc (Test Acc): 19.900 (20.623)\n",
      "Generation # 6000. Train Loss (Test Loss): 2.435 (2.424). Train Acc (Test Acc): 19.000 (20.605)\n",
      "Generation # 6500. Train Loss (Test Loss): 2.410 (2.410). Train Acc (Test Acc): 21.000 (20.614)\n",
      "Generation # 7000. Train Loss (Test Loss): 2.389 (2.396). Train Acc (Test Acc): 21.500 (20.619)\n",
      "Generation # 7500. Train Loss (Test Loss): 2.379 (2.384). Train Acc (Test Acc): 21.200 (20.600)\n",
      "Generation # 8000. Train Loss (Test Loss): 2.339 (2.373). Train Acc (Test Acc): 22.200 (20.600)\n",
      "Generation # 8500. Train Loss (Test Loss): 2.372 (2.363). Train Acc (Test Acc): 19.400 (20.600)\n",
      "Generation # 9000. Train Loss (Test Loss): 2.364 (2.354). Train Acc (Test Acc): 21.300 (20.619)\n",
      "Generation # 9500. Train Loss (Test Loss): 2.352 (2.346). Train Acc (Test Acc): 22.600 (20.623)\n",
      "Generation # 10000. Train Loss (Test Loss): 2.370 (2.339). Train Acc (Test Acc): 19.700 (20.628)\n",
      "Generation # 10500. Train Loss (Test Loss): 2.334 (2.332). Train Acc (Test Acc): 21.400 (20.623)\n",
      "Generation # 11000. Train Loss (Test Loss): 2.323 (2.327). Train Acc (Test Acc): 21.200 (20.623)\n",
      "Generation # 11500. Train Loss (Test Loss): 2.339 (2.322). Train Acc (Test Acc): 19.900 (20.623)\n",
      "Generation # 12000. Train Loss (Test Loss): 2.282 (2.317). Train Acc (Test Acc): 22.900 (20.623)\n",
      "Generation # 12500. Train Loss (Test Loss): 2.326 (2.313). Train Acc (Test Acc): 19.300 (20.623)\n",
      "Generation # 13000. Train Loss (Test Loss): 2.339 (2.309). Train Acc (Test Acc): 20.200 (20.609)\n",
      "Generation # 13500. Train Loss (Test Loss): 2.280 (2.306). Train Acc (Test Acc): 21.900 (20.609)\n",
      "Generation # 14000. Train Loss (Test Loss): 2.337 (2.303). Train Acc (Test Acc): 18.500 (20.600)\n",
      "Generation # 14500. Train Loss (Test Loss): 2.300 (2.300). Train Acc (Test Acc): 21.600 (20.600)\n",
      "Generation # 15000. Train Loss (Test Loss): 2.295 (2.298). Train Acc (Test Acc): 21.500 (20.600)\n",
      "Generation # 15500. Train Loss (Test Loss): 2.278 (2.296). Train Acc (Test Acc): 20.000 (20.600)\n",
      "Generation # 16000. Train Loss (Test Loss): 2.305 (2.293). Train Acc (Test Acc): 20.500 (20.600)\n",
      "Generation # 16500. Train Loss (Test Loss): 2.290 (2.292). Train Acc (Test Acc): 24.000 (20.591)\n",
      "Generation # 17000. Train Loss (Test Loss): 2.304 (2.290). Train Acc (Test Acc): 19.500 (20.595)\n",
      "Generation # 17500. Train Loss (Test Loss): 2.286 (2.288). Train Acc (Test Acc): 22.500 (20.586)\n",
      "Generation # 18000. Train Loss (Test Loss): 2.321 (2.286). Train Acc (Test Acc): 20.000 (20.586)\n",
      "Generation # 18500. Train Loss (Test Loss): 2.297 (2.285). Train Acc (Test Acc): 21.600 (20.591)\n",
      "Generation # 19000. Train Loss (Test Loss): 2.291 (2.284). Train Acc (Test Acc): 19.100 (20.591)\n",
      "Generation # 19500. Train Loss (Test Loss): 2.312 (2.282). Train Acc (Test Acc): 20.700 (20.591)\n",
      "Generation # 20000. Train Loss (Test Loss): 2.276 (2.281). Train Acc (Test Acc): 21.900 (20.591)\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import math\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os.path\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()   \n",
    "\n",
    "num_classes = len(CATEGORIES)\n",
    "\n",
    "current_log_dir = os.path.join(log_dir, datetime.now().strftime(\"%Y-%m-%d-%H_%M_%S\"))\n",
    "tf.gfile.MakeDirs(current_log_dir)\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    x_data = tf.placeholder(shape=[None, num_features], dtype=tf.float32)\n",
    "    y_target = tf.placeholder(tf.int32, shape=(None))\n",
    "\n",
    "    keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    \n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights1 = tf.Variable(\n",
    "            tf.truncated_normal([num_features, num_hidden1],\n",
    "                            stddev=1.0 / math.sqrt(float(num_features))),\n",
    "                            name='weights')\n",
    "        biases1 = tf.Variable(tf.zeros([num_hidden1]),\n",
    "                             name='biases')\n",
    "        hidden_relu1 = tf.nn.relu(tf.matmul(x_data, weights1) + biases1)\n",
    "        hidden1 = tf.nn.dropout(hidden_relu1, keep_prob, name = 'dropout')\n",
    "\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights2 = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden1, num_hidden2],\n",
    "                            stddev=1.0 / math.sqrt(float(num_hidden1))),\n",
    "                            name='weights')\n",
    "        biases2 = tf.Variable(tf.zeros([num_hidden2]),\n",
    "                             name='biases')\n",
    "        hidden_relu2 = tf.nn.relu(tf.matmul(hidden1, weights2) + biases2)\n",
    "        hidden2 = tf.nn.dropout(hidden_relu2, keep_prob, name = 'dropout')\n",
    "\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights_sm = tf.Variable(\n",
    "            tf.truncated_normal([num_hidden2, num_classes],\n",
    "                            stddev=1.0 / math.sqrt(float(num_hidden2))),\n",
    "                            name='weights')\n",
    "        biases_sm = tf.Variable(tf.zeros([num_classes]),\n",
    "                             name='biases')\n",
    "        logits = tf.matmul(hidden2, weights_sm) + biases_sm\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=y_target,\n",
    "            logits=logits,\n",
    "            name='xentropy')\n",
    "        loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "    with tf.name_scope('train'):\n",
    "        # optimizer = tf.train.AdamOptimizer(0.00025)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(0.00025)\n",
    "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)\n",
    "\n",
    "    with tf.name_scope('accuracy1'):\n",
    "        prediction1 = tf.nn.in_top_k(logits, y_target, 1)\n",
    "        accuracy1 = tf.reduce_mean(tf.cast(prediction1, tf.float32))\n",
    "        tf.summary.scalar('accuracy1', accuracy1)\n",
    "    \n",
    "    with tf.name_scope('accuracy2'):\n",
    "        prediction2 = tf.nn.in_top_k(logits, y_target, 2)\n",
    "        accuracy2 = tf.reduce_mean(tf.cast(prediction2, tf.float32))\n",
    "        tf.summary.scalar('accuracy2', accuracy2)\n",
    "    \n",
    "    summary = tf.summary.merge_all()\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter(current_log_dir + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(current_log_dir + '/test')\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    sess.run(init)\n",
    "    \n",
    "    best_result = 0.0\n",
    "    max_steps = min_steps\n",
    "    i = 0\n",
    "    while i < max_steps:\n",
    "        rand_index = np.random.choice(features_train.shape[0], size=batch_size)\n",
    "        rand_x = features_train[rand_index]\n",
    "        rand_y = np.transpose([target_train[rand_index]]).ravel()\n",
    "        feed_dict = {\n",
    "            x_data: rand_x,\n",
    "            y_target: rand_y,\n",
    "            keep_prob: 0.5,\n",
    "        }\n",
    "        \n",
    "        sess.run(train_op, feed_dict=feed_dict)\n",
    "\n",
    "        # Only record loss and accuracy every 100 generations\n",
    "        if (i+1)%100==0:\n",
    "            feed_dict_train = {\n",
    "                x_data: rand_x,\n",
    "                y_target: rand_y,\n",
    "                keep_prob: 1.0,\n",
    "            }\n",
    "\n",
    "            feed_dict_test = {\n",
    "                x_data: features_test,\n",
    "                y_target: np.transpose([target_test]).ravel(),\n",
    "                keep_prob: 1.0,\n",
    "            }\n",
    "\n",
    "            train_loss_temp = sess.run(loss, feed_dict=feed_dict_train)\n",
    "            train_acc_temp = sess.run(accuracy1, feed_dict=feed_dict_train)\n",
    "            train_acc2_temp = sess.run(accuracy2, feed_dict=feed_dict_train)\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict_train)\n",
    "            train_writer.add_summary(summary_str, i)\n",
    "            train_writer.flush()\n",
    "\n",
    "            test_loss_temp = sess.run(loss, feed_dict=feed_dict_test)\n",
    "            test_acc_temp = sess.run(accuracy1, feed_dict=feed_dict_test)\n",
    "            test_acc2_temp = sess.run(accuracy2, feed_dict=feed_dict_train)\n",
    "            summary_str = sess.run(summary, feed_dict=feed_dict_test)\n",
    "            test_writer.add_summary(summary_str, i)\n",
    "            test_writer.flush()\n",
    "            \n",
    "        if (i+1)%500==0:\n",
    "            acc_and_loss = [i+1, train_loss_temp, test_loss_temp, train_acc_temp * 100, test_acc_temp * 100]\n",
    "            acc_and_loss = [np.round(x,3) for x in acc_and_loss]\n",
    "            print('Generation # {}. Train Loss (Test Loss): {:.3f} ({:.3f}). Train Acc (Test Acc): {:.3f} ({:.3f})'.format(*acc_and_loss))\n",
    "\n",
    "            saver.save(sess, current_log_dir + '/model.ckpt', global_step=i)\n",
    "\n",
    "            if test_acc_temp > best_result:\n",
    "                best_result = test_acc_temp\n",
    "                max_steps = np.max([max_steps, i * 2.0])\n",
    "                print(\"Accuracy improved, now steps count is %d\" % max_steps)\n",
    "            \n",
    "        i += 1\n",
    "    \n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<subprocess.Popen at 0x7f5a0bdaa390>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import subprocess\n",
    "subprocess.Popen(['notify-send', 'Computing complete'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
